# Understanding Recurrent Neural Networks

This repository is simply me working through [CS224d: Deep Learning
for Natural Language Processing](http://cs224d.stanford.edu) (Spring
2016).

There is nothing to see here.

But if youâ€™re even vaguely interested in this topic, you should
probably take this class.

[Harish Narayanan](https://harishnarayanan.org/), 2017

## Course Syllabus

- [ ] **Lecture 1:** Intro to NLP and Deep Learning
   - [x] [Video](https://youtu.be/Qy0oEkCZkBI)
   - [ ] [Slides](slides/lecture1.pdf)
   - [ ] [Notes](notes/lecture1.pdf)
   - [ ] Suggested Reading
      - [ ] [Linear Algebra Review](notes/cs229-linalg.pdf)
      - [ ] [Probability Review](notes/cs229-prob.pdf)
      - [ ] [Convex Optimization Review](notes/cs229-cvxopt.pdf)
      - [x] [More Optimization (SGD) Review](https://github.com/hnarayanan/CS231n/raw/master/notes/optimization.pdf)
      - [ ] [From Frequency to Meaning: Vector Space Models of Semantics](papers/live-2934-4846-jair.pdf)
      - [x] [Python Tutorial](https://github.com/hnarayanan/CS231n/raw/master/notes/python-numpy-tutorial.pdf)
- [ ] **Lecture 2:** Simple Word Vector representations: word2vec, GloVe
   - [ ] [Video](https://youtu.be/aRqn8t1hLxs)
   - [ ] [Slides](slides/lecture2.pdf)
   - [ ] Suggested Reading
      - [ ] [Distributed Representations of Words and Phrases and their Compositionality](papers/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)
      - [ ] [Efficient Estimation of Word Representations in Vector Space](papers/1301.3781.pdf)
